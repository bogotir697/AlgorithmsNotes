#### [[Вопросы к Экзамену]]
---
#### Рекурентные отношения
Рекурсивными называют такие алгортмы, которые в ходе работы вызывают сами себя с другими аргументами. Зачастую, размер данных для обработки при этом уменьшается. Это свойство лежит в основе концепции «Разделяй и властвуй». Для них в алгоритме можно выделить 3 основных шага:
- Разделение задачи на несколько подзадач, которые представляют собой меньшие экземпляры той же задачи.
- Обработка подзадач путём их рекурсивного решения. Если размеры подзадач малы, то такие подзадачи решаются непосредственно.
- Комбинирование подзадач в решение исходной задачи.

Возьмём для примера работу двоичного поиска. Здесь время работы будет константным, если n = 1, либо суммой времени работы подзадачи и собственными задачами. В задаче двоичного поиска к собственным задачам относятся, например, получение элемента по нужному индексу, сравнение его с искомым элементом, определение, какую из двух половин выбрать для дальнейшего поиска. Подзадача же на каждом шаге уменьшается вдвое. Всё это можно выразить в виде рекуррентного соотношения.
$$
T(n)=\begin{cases}
	\Theta(1), & \text{when } n = 1 \\
	T(\frac{n}{2}), & \text{when } n > 1
\end{cases}
$$
### Методы определения асимптотической оценки
#### Метод подстановки
Суть метода подстановки заключается в попытке "догадаться", как будет выглядеть оценка функции, после чего методом индукции проверить её на правильность.
Покажем это для двоичного поиска для которого известно, что $T(n)=T(\frac{n}{2})+O(1)$.
Сначала докажем, что в индукции срабатывает переход от предыдущего значения к следующему. Допустим, что $T(n)=O(\log_2{n})$. Тогда необходимо показать, что $T(n)\leq c\log_2{n}$ для правильно подобранного значения $c>0$.
Предположим, что это ограничение уже верно для всех значений $m<n$. Например, для $m=\frac{n}{2}$, тогда $T(\frac{n}{2})\leq c\log_2{\frac{n}{2}}$. Подставляя в исходное рекуррентное соотношение получаем: $$ T(n) = T(\frac{n}{2}) + \Theta(1) \le c \log_2⁡{\frac{n}{2}} + d = c \log_2⁡{n} - c + d \le c \log_2⁡{n} $$
Последнее неравенство выполняется при c ≥ 1 и d ≤ c. А значит шаг индукции доказан.
Теперь необходимо найти базис индукции. Поскольку $T(1) = d$, а $c \log_2⁡ 1 = 0$ для любого c, выражение $T(1) \le c \log_2⁡ 1$ не является истиной и не может быть базой, если $d > 0$. Воспользуемся тем, что для асимптотических оценок достаточно найти $n_0$ при котором соотношение становится верным: $$T(1) = d, c \log_2 ⁡1 = 0 \Rightarrow d \le 0$$ $$T(2) = 2d, c \log_2 ⁡2 = c \Rightarrow 2d \le c$$$$T(4) = 3d, c \log_2 ⁡4 = 2c \Rightarrow 3d \le 2c$$
Возьмем базис при $n_0 = 2$, тогда при $c = 2d$ индукция будет доказана.
#### Метод дерева рекурсий
В методе дерева рекурсии все вызовы изображаются в виде дерева, для каждого вызова определяется его стоимость, после чего достаточно будет посчитать сумму всех стоимостей.
Рассмотрим дерево рекурсии для двоичного поиска. В нём каждый узел, начиная с корневого, будет порождать только одну ветвь работы алгоритма: алгоритм выбирает либо левую, либо правую половину. Всего таких разбиений будет $\log_2 ⁡n$, т. к. при каждом разбиении размер входных данных уменьшается вдвое. Показать это можно, определив, что на k-ом вызове функции размер входных данных уменьшится ровно в $\frac{n}{2^k}$. Если зафиксировать, что на этом шаге размер данных равен $1$, то решив уравнение получается, что $n = 2^k \Rightarrow k = \log_2 ⁡n$, где $k$ — последний шаг рекурсии и высота дерева.
![[RecursiveTree1.png]]
Просуммировав стоимость каждой операции $dk$ раз определим, что: 
$$T(n) = d \log_2 ⁡n = \Theta(\log_2 ⁡n)$$
Рассмотрим другой пример, где стоимость каждого узла линейно зависит от размера данных, а каждая задача разбивается на 4 подзадачи, в каждую из которых уходит половина данных. Тогда на i-м уровне получаем стоимость обработки подзадачи как $4^i \frac{cn}{2^i}$.
![[RecursiveTree2.png]]
Высота дерева напрямую зависит от того, как быстро уменьшается количество данных. В данном случае на каждом уровне количество данных в 2 раза меньше, а следовательно высота дерева будет ограничена $\log_2 n$. Тогда просуммировав все элементы получим, что:
$$T(n) = cn(2n - 1) = \Theta(n^2)$$
Мы рассмотрели простые случаи, но в реальности деревья могут делиться не на равные части, например, рекуррентное соотношение может выглядеть так:
$$T(n) = 3T(\frac{n}{5}) + 2T(\frac{n}{4}) + T(\frac{n}{3})$$
#### Мастер теорема
Самым простым методом определения времени работы считается использование мастер-теоремы. Суть которого в том, что рекуррентное соотношение можно представить в общем виде как:
$$T(n)=aT(\frac{n}{b})+f(n)$$

где $a\geq1$, $b > 1$, а $T(n)$ — заданная функция. Здесь $a$ означает количество подзадач, на которое делится исходная подзадача, а $b$ — во сколько раз уменьшается количество данных при вызове рекурсивной функции. $f(n)$ — это работа по обработке $1$ вызова метода рекурсии. Далее ответ зависит от одного из трёх случаев, с помощью которого асимптотические оценки определяются достаточно просто:

- Если $f(n) = O(n^{\log_b⁡ a - \epsilon})$ для некоторого $\epsilon > 0$, то $T(n) = \Theta(n^{\log_b⁡ a})$
- Если $f(n) = \Theta(n^{\log_b ⁡a})$, то $T(n) = \Theta(n^{\log_b a} \log_2 ⁡n)$
- Если $f(n) = \Omega(n^{log_b⁡ a + \epsilon})$ для некоторого $\epsilon > 0$ и если $a f(\frac{n}{b}) \le c f(n)$ для некоторой $c < 1$, то $T(n) = \Theta(f(n))$
Рассмотрим её применения для двоичного поиска:
$$T(n) = T(\frac{n}{2}) + d$$
Поскольку $d = \Theta(n^{\log_2⁡ 1}) = \Theta(n^0) = \Theta(1)$, то срабатывает второе правило мастер-теоремы и ответом является:
$$T(n) = \Theta(n^{\log_2 ⁡1} log_2 ⁡n) = \Theta(\log_2 ⁡n)$$
Рассмотрим другой пример рассмотренный раннее. Пусть:
$$T(n) = 4T(\frac{n}{2}) + kn$$
* Попробуем применить 3-е правило мастер теоремы, где $a = 4$, $b = 2$ и проверить утверждение, что: $f(n) = \Omega(n^{\log_2 {4 + \epsilon}})$ Но, поскольку функция $kn$ не может быть оценена снизу, т.е. $kn \not= \Omega(n^{\log_2 {4 + \epsilon}})$ для любого $\epsilon > 0$, то это правило не может быть применено.
* Попробуем применить 2-ё правило мастер теоремы, тогда: $f(n) = \Theta(n^{\log_2 4})$, но и в этом случае оно не работает, т.к. $kn \not= \Theta(n^2)$.
* Рассмотрим первое правило $f(n) = O(n^{\log_2⁡ {4 - \epsilon}})$. В этом случае $kn = O(n^{2 - \epsilon})$ при $0 < \epsilon \leq 1$. Тогда $T(n) = \Theta(n^{\log_b⁡ a}) = \Theta(n^2)$.