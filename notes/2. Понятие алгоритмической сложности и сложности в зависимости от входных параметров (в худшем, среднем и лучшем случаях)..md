#### [[Вопросы к Экзамену]]
---
### Алгоритмическая сложность
> Это количество операций, необходимое для выполнения алгоритма, в зависимости от размера данных

Зачем вообще нужна понимать, какова сложность алгоритма? Дело в том, что она напрямую связана с временем выполнения, и если можно выполнить задачу эффективнее, то почему бы этого не сделать. Есть пример, который хорошо проиллюстрирует важность этого понятия:
Допустим есть два разных алгоритма, решающих одну и ту же задачу. Первый алгоритм имеет сложность $2n^2$, а второй — $50{n}\log_2{n}$. Сравним время их выполнения на двух разных компьютерах: **Компьютер А**, способный выполнять $10^{10}$ команд в секунду, и **Компьютер Б**, выполняющий $10^{7}$ команд в секунду. Запустим первый алгоритм на компьютере **А**, а второй — на компьютере Б. Пусть $n = 10^{8}$.
> **Компьютер А**, быстрый, $2n^2$
$$\frac{2*(10^7)^2 \text{ команд}}{10^{10}\text{ команд в секунду}} = \text{20 000 секунд (более 5,5 часов)}$$
> **Компьютер Б**, медленный, $50nlog_2{n}$
$$\frac{50*10^7*\log_2{10^7} \text{ команд}}{10^{7} \text{ команд в секунду}} = \text{1163 секунды (менее 20 минут)}$$

Как можно заметить, сложность алгоритма имеет намного большее влиянее на его время выполнения, чем скорость компьютера.
Рост целевой функции $T()n$ имеет различные названия:

1. Константное (время работы не зависит от n)
2. Логарифмическое ($\log{n}$: напр., двоичный поиск)
3. Линейное ($n$: напр., поиск максимального значения)
4. Квазилинейное ($n\log{n}$: напр., большинство эффективных сортировок)
5. Квадратичное ($n^2$: напр., обход значений матрицы)
6. Полиномиальное ($n^c$ для c>1)
7. Экспоненциальное ($c^n$ для c>1)
8. Факториальное ($n!$: напр., задача коммивояжёра)
При определённом наборе данных алгоритм может иметь разную временную сложность. Так, к примеру, простая реализация сортировки пузырьком работает за квадратичное время, то есть за $n^2$ для всех входных данных. Но при простой модификации, которая проверяет, что входной массив данных уже отсортирован, достаточно одного прохода по всем элементам. В таком случае говорят, что алгоритм работает за линейное время в лучшем и за квадратичное время в худшем случае.
Иногда посчитать работу алгоритма в худшем или лучшем случае не представляется возможным. Тогда для оценки скорости роста функции применяют подсчёт в среднем.
Для выполнения подсчёта в среднем используется вероятностный анализ алгоритма. Суть которого заключается в определении всех вероятностей получить на вход какие-то входные значения заданного размера n, после чего рассчитывается математическое ожидание соответствующей случайной величины, как сумма произведения времени работы определённого набора величин на его вероятность.
Обозначим время работы некоторого алгоритма $A$, на вход которого подаётся вход $x$, как $C_A(x)$. Для определения среднего случая рассмотрим конечное множество $X_n=\{x : ||x|| = n\}$ входов размеров $n$. Предположим, что каждому входу $x \in X_n$ приписана вероятность $P_n(x)$. На заданном таким образом вероятностном пространстве сложностью в среднем называют математическое ожидание соответствующей случайной величины:
$$T_A(n) = \sum_{x \in X_n} C_A(x)P_n(x)$$
Например, для алгоритма двоичного (бинарного) поиска среднее время работы можно определить, следуя следующим соображениям: вероятность найти элемент на первом шаге равна $^1/_n$ , где $n$ — длина отсортированной последовательности. Если на первом шаге элемент не найден, то найти его в подпоследовательности размером $^n/_2$ равyна $^2/_n$ , в подподследовательности размером $^n/_4$ соответственно $^4/_n$ . Тогда вероятность обнаружить элемент на $i$-м шаге будет $\frac{2^{i - 1}}{n}$. Максимум таких шагов может быть $\log_2 n$, тогда среднее количество шагов которое нужно сделать для того, чтобы найти элемент будет:
$$T(n) = \sum_{x=1}^{\log_2 n} i\frac{2^{i - 1}}{n} = \frac{1}{n}\sum_{x=1}^{\log_2 n} i 2^{i - 1}$$
$$\sum_{i=1}^{\log_2 n} i 2^{i - 1} = n \log_2 n - n + 1$$
$$T(n) = \frac{n \log_2 n - n + 1}{n} \approx \log_2 n$$
